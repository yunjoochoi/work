# pip install nvidia-ml-py
import time
import torch
import gc
from pynvml import *
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.pipeline_options import PdfPipelineOptions, AcceleratorOptions
from docling.datamodel.base_models import InputFormat
from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend

def get_real_vram_usage(pid):
    """NVIDIA ë“œë¼ì´ë²„ì—ì„œ í•´ë‹¹ í”„ë¡œì„¸ìŠ¤(PID)ì˜ ì‹¤ì œ VRAM ì ìœ ìœ¨ì„ ì¡°íšŒ"""
    try:
        handle = nvmlDeviceGetHandleByIndex(0) # 0ë²ˆ GPU ê¸°ì¤€
        info = nvmlDeviceGetComputeRunningProcesses(handle)
        for process in info:
            if process.pid == pid:
                return process.usedGpuMemory / 1024**2 # MB ë‹¨ìœ„ ë°˜í™˜
    except Exception:
        pass
    return 0

def print_status(step):
    """í˜„ì¬ í”„ë¡œì„¸ìŠ¤ì˜ PyTorch ì¸¡ì •ê°’ vs ì‹¤ì œ ë“œë¼ì´ë²„ ì¸¡ì •ê°’ ë¹„êµ"""
    torch.cuda.synchronize()
    pid = os.getpid()
    
    # 1. PyTorchê°€ ì•Œê³  ìˆëŠ” ê°’
    torch_allocated = torch.cuda.memory_allocated() / 1024**2
    torch_reserved = torch.cuda.memory_reserved() / 1024**2
    
    # 2. ì‹¤ì œ NVIDIA ë“œë¼ì´ë²„ê°€ ë§í•˜ëŠ” ê°’ (ì§„ì‹¤)
    real_usage = get_real_vram_usage(pid)
    
    print(f"[{step}]")
    print(f"  - PyTorch Allocated (ìˆœìˆ˜ ëª¨ë¸): {torch_allocated:,.1f} MB")
    print(f"  - PyTorch Reserved  (ìºì‹œ í¬í•¨): {torch_reserved:,.1f} MB")
    print(f"  ğŸ”¥ Real Process Usage (ì° ì‚¬ìš©ëŸ‰): {real_usage:,.1f} MB (CUDA Context í¬í•¨)")
    print("-" * 50)
    return real_usage

def main():
    nvmlInit()
    print(f"ğŸš€ ì¸¡ì • ì‹œì‘ (PID: {os.getpid()})...\n")
    
    # 1. ì´ˆê¸° ìƒíƒœ (ì•„ë¬´ê²ƒë„ ì•ˆ í•¨)
    # ì£¼ì˜: torch.cuda.is_available()ë§Œ í˜¸ì¶œí•´ë„ CUDA Contextê°€ ìƒì„±ë˜ì–´ ìˆ˜ë°± MBê°€ ì¡í ìˆ˜ ìˆìŒ
    base_usage = print_status("1. ì´ˆê¸° ìƒíƒœ")

    # 2. ëª¨ë¸ ë¡œë”©
    print("Docling ëª¨ë¸ ë¡œë”© ì¤‘...")
    pipeline_options = PdfPipelineOptions()
    pipeline_options.do_ocr = False
    pipeline_options.do_table_structure = True
    pipeline_options.generate_picture_images = True
    pipeline_options.images_scale = 2.0
    pipeline_options.layout_batch_size = 32
    pipeline_options.table_batch_size = 32

    pipeline_options.accelerator_options = AcceleratorOptions(
        num_threads=4, device=torch.device("cuda")
    )

    converter = DocumentConverter(
        format_options={
            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
        }
    )

    # Fallback converter with PyPdfiumDocumentBackend for handling "Invalid code point" errors
    fallback_converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(
                    pipeline_options=pipeline_options,
                    backend=PyPdfiumDocumentBackend
                )
            }
        )
    load_usage = print_status("2. ëª¨ë¸ ë¡œë”© ì§í›„")

    # 3. (ì¤‘ìš”) ë”ë¯¸ ì¸í¼ëŸ°ìŠ¤ ì‹¤í–‰ (í”¼í¬ ë©”ëª¨ë¦¬ í™•ì¸ìš©)
    # ì‹¤ì œ PDF ì²˜ëŸ¼ ë™ì‘í•˜ê²Œ í•˜ì—¬ ìˆœê°„ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ê°€ ì–¼ë§ˆë‚˜ íŠ€ëŠ”ì§€ í™•ì¸
    print("ğŸƒ ë”ë¯¸ ì¸í¼ëŸ°ìŠ¤(Warmup) ì‹¤í–‰ ì¤‘... (í”¼í¬ ë©”ëª¨ë¦¬ í™•ì¸)")
    try:
        # ë¹ˆ PDFë‚˜ ê°„ë‹¨í•œ URL ë“±ìœ¼ë¡œ ì‹¤ì œ íŒŒì´í”„ë¼ì¸ì„ íƒœì›Œë´ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” ëª¨ë¸ì´ ë©”ëª¨ë¦¬ì— 'ìë¦¬ ì¡ê²Œ' í•˜ëŠ” ìš©ë„
        pass 
        # (ì‹¤ì œ íŒŒì¼ì„ ë„£ì–´ì„œ convert()ë¥¼ í•œ ë²ˆ ì‹¤í–‰í•˜ëŠ” ì½”ë“œë¥¼ ë„£ìœ¼ë©´ ë² ìŠ¤íŠ¸ì…ë‹ˆë‹¤)
    except:
        pass
        
    final_usage = print_status("3. ì¸í¼ëŸ°ìŠ¤ ì¤€ë¹„ ì™„ë£Œ ìƒíƒœ")

    print("\nğŸ“Š [ìµœì¢… ê²°ë¡ ]")
    print(f"âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ í¬ê¸°: ì•½ {load_usage - base_usage:,.1f} MB")
    print(f"âœ… í”„ë¡œì„¸ìŠ¤ 1ê°œë‹¹ í•„ìš” VRAM: ìµœì†Œ {final_usage:,.1f} MB")
    print(f"   (CUDA Context + ëª¨ë¸ + ë¼ì´ë¸ŒëŸ¬ë¦¬ ì˜¤ë²„í—¤ë“œ í¬í•¨)")

    nvmlShutdown()

if __name__ == "__main__":
    main()