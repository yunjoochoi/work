#  DocTool í´ë˜ìŠ¤ì™€ ì‹¤í–‰ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •

class DocTool:
    """
    High-level document processing tool with Multi-GPU/CPU support.
    Now supports Streaming output.
    """

    def __init__(
        self,
        do_ocr: bool = False,
        do_table_structure: bool = True,
        chunk_page_size: int = 10,
        worker_restart_interval: int = 20,
        cpu_workers: int = 4,
    ):
        self.config = ParserConfig(
            do_ocr=do_ocr,
            do_table_structure=do_table_structure,
            chunk_page_size=chunk_page_size,
            worker_restart_interval=worker_restart_interval,
            cpu_workers=cpu_workers,
        )

    def stream(self, file_dict: Dict[str, BytesIO]):
        """
        Process multiple documents and yield results as soon as they are ready.
        (Generator function)
        
        Args:
            file_dict: Dictionary mapping filenames to BytesIO objects
            
        Yields:
            Document objects one by one.
        """
        num_gpus = torch.cuda.device_count()
        print(f"[DocTool] Detected {num_gpus} GPU(s)")

        # 1. Worker ì„¤ì •
        if num_gpus > 0:
            num_workers = num_gpus
            gpu_ids = list(range(num_gpus))
            cpus_per_worker = None
        else:
            num_workers = self.config.cpu_workers
            gpu_ids = None
            try:
                allowed_cpus = sorted(os.sched_getaffinity(0))
                total_cpus = len(allowed_cpus)
                cpus_per_worker = max(1, total_cpus // num_workers)
            except Exception:
                cpus_per_worker = None

        # 2. ì‘ì—… ì¤€ë¹„ (Task Preparation)
        all_tasks = []
        
        # íŒŒì¼ë³„ ì²­í¬ ê°œìˆ˜ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ë©”íƒ€ë°ì´í„°
        # key: original_filename, value: { "total": int, "collected": list[ChunkResult] }
        doc_buffers = {} 
        
        # ë‚¨ì€ ì²˜ë¦¬ ëŒ€ìƒ íŒŒì¼ ìˆ˜
        pending_files_count = 0

        for filename, file_stream in file_dict.items():
            ext = Path(filename).suffix.lower()
            file_stream.seek(0)
            file_bytes = file_stream.read()

            if ext == '.pdf':
                # PDFëŠ” ì²­í¬ë¡œ ë¶„í• 
                chunks = _split_pdf_to_chunks(
                    file_id=filename,
                    pdf_bytes=file_bytes,
                    chunk_page_size=self.config.chunk_page_size
                )
                
                doc_buffers[filename] = {
                    "total": len(chunks),
                    "collected": [],
                    "start_time": time.perf_counter()
                }
                
                for chunk_filename, chunk_index, chunk_stream, start_page in chunks:
                    chunk_stream.seek(0)
                    chunk_bytes = chunk_stream.read()
                    all_tasks.append((
                        chunk_filename,
                        chunk_index,
                        filename,    # original_file_id
                        chunk_bytes,
                        file_bytes,  # needed for chart extraction context if applicable
                        start_page   # page_offset
                    ))
            else:
                # PDFê°€ ì•„ë‹Œ íŒŒì¼ì€ í†µì§¸ë¡œ 1ê°œì˜ ì²­í¬ì²˜ëŸ¼ ì²˜ë¦¬
                # Workerê°€ í™•ì¥ìë¥¼ ë³´ê³  ì•Œì•„ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
                doc_buffers[filename] = {
                    "total": 1,
                    "collected": [],
                    "start_time": time.perf_counter()
                }
                
                all_tasks.append((
                    filename,       # chunk_filename (same as original)
                    0,              # chunk_index
                    filename,       # original_file_id
                    file_bytes,     # chunk_bytes (entire file)
                    file_bytes,     # file_bytes
                    0               # page_offset
                ))

            pending_files_count += 1

        total_tasks = len(all_tasks)
        print(f"[DocTool] Total files: {len(file_dict)}, Total tasks (chunks): {total_tasks}")

        # 3. Manager ë° Worker ì‹œì‘
        config_dict = self.config.__dict__.copy() # dataclass to dict
        
        manager = WorkerManager(
            num_workers=num_workers,
            gpu_ids=gpu_ids,
            config_dict=config_dict,
            worker_restart_interval=self.config.worker_restart_interval,
            cpus_per_worker=cpus_per_worker
        )
        manager.start_workers()

        # 4. ì‘ì—… íì— ë„£ê¸°
        for task in all_tasks:
            manager.task_queue.put(task)

        # 5. ê²°ê³¼ ìˆ˜ì§‘ ë° ìŠ¤íŠ¸ë¦¬ë° (Streaming Loop)
        received_chunks = 0
        
        try:
            while pending_files_count > 0:
                # ì›Œì»¤ ìƒì¡´ í™•ì¸ ë° ì¬ì‹œì‘
                for i in range(manager.num_workers):
                    if not manager.processes[i].is_alive():
                        # ì›Œì»¤ê°€ ì£½ì—ˆìœ¼ë©´ ì¬ì‹œì‘ (ì‘ì—…ì´ ë‹¤ ëë‚˜ê¸° ì „ì´ë¼ë©´)
                        # ì£¼ì˜: ì‘ì—… íì— ë‚¨ì•„ìˆëŠ” ì‘ì—…ì€ ì‚´ì•„ìˆëŠ” ì›Œì»¤ê°€ ê°€ì ¸ê°€ì§€ë§Œ, 
                        # ì£½ì€ ì›Œì»¤ê°€ ì²˜ë¦¬ ì¤‘ì´ë˜ ì‘ì—…ì€ ìœ ì‹¤ë  ìˆ˜ ìˆìŒ (ì—¬ê¸°ì„  ë‹¨ìˆœ ì¬ì‹œì‘ë§Œ êµ¬í˜„)
                        gpu_id = manager.gpu_ids[i] if manager.gpu_ids else None
                        print(f"ğŸ”„ [Manager] Worker died, restarting...")
                        manager.restart_worker(i, gpu_id)

                try:
                    # ê²°ê³¼ ëŒ€ê¸°
                    chunk_result = manager.result_queue.get(timeout=5)
                    received_chunks += 1
                    
                    fid = chunk_result.original_file_id
                    buffer = doc_buffers[fid]
                    
                    # ê²°ê³¼ ë²„í¼ì— ì¶”ê°€
                    buffer["collected"].append(chunk_result)
                    
                    # í•´ë‹¹ íŒŒì¼ì˜ ëª¨ë“  ì²­í¬ê°€ ëª¨ì˜€ëŠ”ì§€ í™•ì¸
                    if len(buffer["collected"]) == buffer["total"]:
                        # 1. ë³‘í•© (Merge)
                        merged_doc = self._merge_single_file(fid, buffer["collected"])
                        
                        # 2. ìˆ˜í–‰ ì‹œê°„ ê³„ì‚° ë° ë¡œê·¸
                        elapsed = time.perf_counter() - buffer["start_time"]
                        print(f"âœ… [Yield] {fid} ready ({elapsed:.2f}s)")
                        
                        # 3. ê²°ê³¼ ë°˜í™˜ (Yield)
                        yield merged_doc
                        
                        # 4. ë©”ëª¨ë¦¬ ì •ë¦¬ (ë²„í¼ ì‚­ì œ)
                        del doc_buffers[fid]
                        pending_files_count -= 1
                        
                except Empty:
                    # íƒ€ì„ì•„ì›ƒ ë°œìƒ ì‹œ, ì•„ì§ ì‘ì—…ì´ ë‚¨ì•˜ëŠ”ë° ëª¨ë“  ì›Œì»¤ê°€ ì£½ì—ˆëŠ”ì§€ ì²´í¬
                    alive_workers = sum(1 for p in manager.processes if p.is_alive())
                    if alive_workers == 0 and pending_files_count > 0:
                        print("[DocTool] Critical: All workers are dead but tasks remain.")
                        break
                    continue
                except Exception as e:
                    print(f"[DocTool] Error in streaming loop: {e}")
                    traceback.print_exc()
                    break

        finally:
            # 6. ì¢…ë£Œ ì²˜ë¦¬
            manager.shutdown()
            print("[DocTool] Streaming finished.")

    def _merge_single_file(self, file_id: str, chunks: List[ChunkResult]) -> Document:
        """Merge chunks for a single file into a Document object."""
        # ì²­í¬ ì¸ë±ìŠ¤ ìˆœ ì •ë ¬
        chunks.sort(key=lambda x: x.chunk_index)
        
        # ì—ëŸ¬ ì²´í¬
        failed_chunks = [c for c in chunks if not c.success]
        if failed_chunks:
            print(f"[Warning] {file_id} has {len(failed_chunks)} failed chunks.")

        # í…ìŠ¤íŠ¸ ë³‘í•©
        merged_text = "\n\n".join([c.text for c in chunks if c.success])

        # ì´ë¯¸ì§€ ë³‘í•©
        all_images = []
        for chunk in chunks:
            if chunk.success and chunk.images:
                all_images.extend(chunk.images)

        return Document(
            id=file_id,
            text=merged_text,
            images=all_images if all_images else None
        )


if __name__ == "__main__":
    # Windows/CUDA multiprocessing fix
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except RuntimeError:
        pass

    input_folder = Path("/home/shaush/pdfs") # ê²½ë¡œ ìˆ˜ì • í•„ìš”
    output_root = Path("/home/shaush/work/parsed-outputs")
    log_file_path = output_root / "parsing_log.txt"

    output_root.mkdir(parents=True, exist_ok=True)
    
    # ì…ë ¥ íŒŒì¼ ì½ê¸°
    file_list = [p.resolve() for p in input_folder.iterdir() if p.is_file()]
    print(f"Found {len(file_list)} files.")
    
    file_dict = {}
    for file_path in file_list:
        with open(file_path, "rb") as f:
            file_dict[file_path.name] = BytesIO(f.read())

    # í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = DocTool(
        chunk_page_size=10,
        worker_restart_interval=20,
        cpu_workers=2
    )

    print("Starting Streaming Process...")
    start_time = time.perf_counter()

    # Log íŒŒì¼ ì—´ê¸° (Append ëª¨ë“œ í˜¹ì€ Write ëª¨ë“œ)
    with open(log_file_path, "w", encoding="utf-8") as log_file:
        log_file.write(f"Streaming Processing Started at {time.ctime()}\n")
        
        # stream() ì œë„ˆë ˆì´í„° ìˆœíšŒ
        count = 0
        for doc in processor.stream(file_dict):
            count += 1
            filename = doc.id
            save_path = output_root / (Path(filename).stem + ".md")

            # ê²°ê³¼ ì €ì¥
            try:
                with open(save_path, "w", encoding="utf-8") as f:
                    f.write(doc.text)
                
                # ë¡œê·¸ ê¸°ë¡
                num_images = len(doc.images) if doc.images else 0
                log_msg = f"[{count}] Saved {filename} | Images: {num_images}"
                print(log_msg)
                
                log_file.write(log_msg + "\n")
                log_file.write(f"   - Text len: {len(doc.text)}\n")
                log_file.flush() # ì¦‰ì‹œ íŒŒì¼ì— ì“°ê¸°

            except Exception as e:
                print(f"Failed to save {filename}: {e}")

    total_time = time.perf_counter() - start_time
    print(f"All done! Total time: {total_time:.2f}s")